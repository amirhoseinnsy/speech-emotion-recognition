# Speech Emotion Recognition (Assignment 5)

This repository contains my implementation for **Assignment 5** of the *Deep Learning for Audio and Image Applications* course at the University of Tehran.  
The project focuses on **Speech Emotion Recognition (SER)** using both traditional and modern feature extraction techniques (Log-Mel spectrograms and HuBERT embeddings).

---

## ğŸ“‚ Project Structure

```bash
ASSIGNMENT5/
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ config.yaml # General configuration
â”‚ â”œâ”€â”€ log_hubert.yaml # HuBERT feature extraction setup
â”‚ â””â”€â”€ log_mel.yaml # Mel-spectrogram setup
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ data_loader.py # Handles dataset loading and preprocessing
â”‚ â””â”€â”€ dataset/ # Raw or processed data (CREMA-D)
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ model.py # Neural network architectures (CNN / MLP)
â”‚ â””â”€â”€ saved_models/ # Directory for trained models
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ main.py # Entry point for running the whole pipeline
â”‚ â”œâ”€â”€ train.py # Training script
â”‚ â”œâ”€â”€ evaluate.py # Evaluation and testing
â”‚ â””â”€â”€ optim.py # Optimization utilities
â”‚
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ features.py # Feature extraction (Log-Mel & HuBERT)
â”‚ â”œâ”€â”€ metrics.py # Accuracy, confusion matrix, etc.
â”‚ â”œâ”€â”€ visualization.py # Plot loss curves, confusion matrix, etc.
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ picture/ # Saved plots and visualizations
â”œâ”€â”€ debug_log.txt
â”œâ”€â”€ report.pdf # Written report of the assignment
â””â”€â”€ HWExtra.pdf / Report_template.docx
```

---

## ğŸ§  Task Description

The goal is to build and compare two pipelines for classifying emotions from speech audio:

1. **Traditional approach:**  
   Extract **Log-Mel spectrogram** features using `librosa` or `torchaudio`, then train a CNN-based classifier.

2. **Modern approach (Self-Supervised):**  
   Extract **HuBERT** embeddings from the pre-trained `facebook/hubert-base-ls960` model and train an MLP classifier.

Dataset used: **CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)**  
Classes: *Neutral, Happy, Sad, Angry* (subset for efficiency)

---

## âš™ï¸ Usage

### 1. Setup
```bash
git clone https://github.com/<amirhoseinnsy>/speech-emotion-recognition.git
cd speech-emotion-recognition
pip install -r requirements.txt
```

2. Configuration

Edit parameters in config/config.yaml (dataset path, batch size, learning rate, etc.).
3. Run training

```bash
python scripts/train.py
```

4. Evaluate model

```bash
python scripts/evaluate.py
```

5. Visualize results

Plots are saved under /picture and include training loss, accuracy, and confusion matrix.
ğŸ“Š Expected Results
Model Type	Feature	Accuracy (Validation)
CNN	Log-Mel	~70â€“75%
MLP	HuBERT	~80â€“85%
ğŸ“š References

    CREMA-D Dataset: Kaggle

HuBERT: Hidden-Unit BERT Paper

    Course: Deep Learning for Audio and Image Applications â€” Dr. Rashad Hosseini, University of Tehran
